{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRHmSyHxEIhN"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JM7hDSNClfoK"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(2020)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c8o1FHzD-_y_"
   },
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities \n",
    "def plot_loss(history, label, n):\n",
    "  # Use a log scale to show the wide range of values.\n",
    "    plt.semilogy(history.epoch,  history.history['loss'],\n",
    "               color=colors[n], label='Train '+label)\n",
    "    plt.semilogy(history.epoch,  history.history['val_loss'],\n",
    "          color=colors[n], label='Val '+label,\n",
    "          linestyle=\"--\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend()\n",
    "    \n",
    "def plot_metrics(history):\n",
    "    metrics =  ['loss', 'auc', 'precision', 'recall']\n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace(\"_\",\" \").capitalize()\n",
    "        plt.subplot(2,2,n+1)\n",
    "        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
    "        plt.plot(history.epoch, history.history['val_'+metric],\n",
    "                 color=colors[0], linestyle=\"--\", label='Val')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(name)\n",
    "        plt.legend()\n",
    "\n",
    "def plot_cm(labels, predictions, p=0.5):\n",
    "    cm = confusion_matrix(labels, predictions > p)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
    "    print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "    print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
    "    print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
    "    print('Total Fraudulent Transactions: ', np.sum(cm[1]))\n",
    "    \n",
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "    fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "\n",
    "    plt.plot(100*fp, 100*tp, label=name+ ' (AUC = %0.3f)' % auc(fp, tp), linewidth=2, **kwargs)\n",
    "    plt.xlabel('False positives [%]')\n",
    "    plt.ylabel('True positives [%]')\n",
    "#     plt.xlim([-0.5,20])\n",
    "#     plt.ylim([80,100.5])\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "def AUCcalc(y_val_val, y_pred):\n",
    "    fpr, tpr, thresholds = roc_curve(y_val_val, y_pred)\n",
    "    tauc = auc(fpr, tpr)\n",
    "    return tauc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z3iZVjziKHmX"
   },
   "source": [
    "## Data processing and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_preprocessed_train = pd.DataFrame(pd.read_csv(os.getcwd() + '/' + 'full_preprocessed_train.csv', sep=','))\n",
    "full_preprocessed_val = pd.DataFrame(pd.read_csv(os.getcwd() + '/' + 'full_preprocessed_dev.csv', sep=','))\n",
    "\n",
    "full_preprocessed_train = full_preprocessed_train.replace(np.nan, \" \")\n",
    "full_preprocessed_val = full_preprocessed_val.replace(np.nan, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_train = pd.DataFrame(pd.read_csv(os.getcwd() + '/' + 'dv_train.csv', sep=',', header=None))\n",
    "doc2vec_val = pd.DataFrame(pd.read_csv(os.getcwd() + '/' + 'dv_val.csv', sep=',', header=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =  pd.DataFrame(pd.read_csv(os.getcwd() + '/' + 'test_clean.csv', sep=','))\n",
    "test = test.replace(np.nan, \" \")\n",
    "dv_test = pd.DataFrame(pd.read_csv(os.getcwd() + '/' + 'dv_test.csv', sep=',', header=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_X_training_data = pd.concat([full_preprocessed_train, doc2vec_train], axis=1, sort=False)\n",
    "doc2vec_X_val_data = pd.concat([full_preprocessed_val, doc2vec_val], axis=1, sort=False)\n",
    "test = pd.concat([test, dv_test], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xWKB_CVZFLpB"
   },
   "source": [
    "### Examine the class label imbalance\n",
    "\n",
    "Let's look at the dataset imbalance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HCJFrtuY2iLF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "    Total: 250874\n",
      "    Positive: 25819 (10.29% of total)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "neg, pos = np.bincount(doc2vec_X_training_data['label'])\n",
    "total = neg + pos\n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, pos, 100 * pos / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6qox6ryyzwdr"
   },
   "source": [
    "### Clean and normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ef42jTuxEjnj"
   },
   "outputs": [],
   "source": [
    "cleaned_train = doc2vec_X_training_data.copy()\n",
    "cleaned_val = doc2vec_X_val_data.copy()\n",
    "cleaned_test = test.copy()\n",
    "\n",
    "# You don't want the `Time` column.\n",
    "cleaned_train = cleaned_train.drop(['date'], axis=1)\n",
    "cleaned_val = cleaned_val.drop(['date'], axis=1)\n",
    "cleaned_test = cleaned_test.drop(['date'], axis=1)\n",
    "\n",
    "# The `length` column covers a huge range. Convert to log-space.\n",
    "eps=0.001 \n",
    "cleaned_train['length'] = np.log(cleaned_train.pop('length')+eps)\n",
    "cleaned_val['length'] = np.log(cleaned_val.pop('length')+eps)\n",
    "cleaned_test['length'] = np.log(cleaned_test.pop('length')+eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can't normalize str col\n",
    "cleaned_train = cleaned_train.drop(['review'], axis = 1)\n",
    "cleaned_val = cleaned_val.drop(['review'], axis = 1)\n",
    "cleaned_test = cleaned_test.drop(['review'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_test = cleaned_test.drop(['label'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xfxhKg7Yr1-b"
   },
   "outputs": [],
   "source": [
    "# Form np arrays of labels and features.\n",
    "train_df, test_df = cleaned_train, cleaned_test\n",
    "val_df = cleaned_val\n",
    "\n",
    "train_labels = np.array(train_df.pop('label'))\n",
    "val_labels = np.array(val_df.pop('label'))\n",
    "\n",
    "train_features = np.array(train_df)\n",
    "val_features = np.array(val_df)\n",
    "test_features = np.array(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8a_Z_kBmr7Oh"
   },
   "source": [
    "Normalize the input features using the sklearn StandardScaler.\n",
    "This will set the mean to 0 and standard deviation to 1.\n",
    "\n",
    "Note: The `StandardScaler` is only fit using the `train_features` to be sure the model is not peeking at the validation or test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IO-qEUmJ5JQg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape: (250874,)\n",
      "Validation labels shape: (35918,)\n",
      "Training features shape: (250874, 145)\n",
      "Validation features shape: (35918, 145)\n",
      "Test features shape: (72165, 145)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "val_features = scaler.transform(val_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "train_features = np.clip(train_features, -5, 5)\n",
    "val_features = np.clip(val_features, -5, 5)\n",
    "test_features = np.clip(test_features, -5, 5)\n",
    "\n",
    "\n",
    "print('Training labels shape:', train_labels.shape)\n",
    "print('Validation labels shape:', val_labels.shape)\n",
    "#print('Test labels shape:', test_labels.shape)\n",
    "\n",
    "print('Training features shape:', train_features.shape)\n",
    "print('Validation features shape:', val_features.shape)\n",
    "print('Test features shape:', test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qFK1u4JX16D8"
   },
   "source": [
    "## Define the model and metrics\n",
    "\n",
    "Define a function that creates a simple neural network with a densly connected hidden layer, a [dropout](https://developers.google.com/machine-learning/glossary/#dropout_regularization) layer to reduce overfitting, and an output sigmoid layer that returns the probability of a transaction being fraudulent: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3JQDzUqT3UYG"
   },
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "def make_model(metrics = METRICS, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    # Sequential groups a linear stack of layers into a tf.keras.Model.\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(64, activation='relu', input_shape=(train_features.shape[-1],)),\n",
    "        keras.layers.Dropout(0.5),\n",
    "#         keras.layers.Dense(64, activation='relu'),\n",
    "#         keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias),])\n",
    "\n",
    "    model.compile(\n",
    "      optimizer=keras.optimizers.Adam(lr=.1),\n",
    "      loss=keras.losses.BinaryCrossentropy(),\n",
    "      metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ouUkwPcGQsy3"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 10000\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0EJj9ixKVBMT"
   },
   "source": [
    "### Checkpoint the initial weights\n",
    "\n",
    "To make the various training runs more comparable, keep this initial model's weights in a checkpoint file, and load them into each model before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_tSUm4yAVIif"
   },
   "outputs": [],
   "source": [
    "initial_bias = np.log([pos/neg])\n",
    "model = make_model(output_bias = initial_bias)\n",
    "initial_weights = os.path.join(tempfile.mkdtemp(),'initial_weights')\n",
    "model.save_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RsA_7SEntRaV"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cveQoiMyGQCo"
   },
   "source": [
    "## Class weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ePGp6GUE1WfH"
   },
   "source": [
    "### Calculate class weights\n",
    "\n",
    "The goal is to identify fradulent transactions, but you don't have very many of those positive samples to work with, so you would want to have the classifier heavily weight the few examples that are available. You can do this by passing Keras weights for each class through a parameter. These will cause the model to \"pay more attention\" to examples from an under-represented class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qjGWErngGny7"
   },
   "outputs": [],
   "source": [
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "weight_for_1 = (1 / pos)*(total)/2.0\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mk1OOE2ZSHzy"
   },
   "source": [
    "### Train a model with class weights\n",
    "\n",
    "Now try re-training and evaluating the model with class weights to see how that affects the predictions.\n",
    "\n",
    "Note: Using `class_weights` changes the range of the loss. This may affect the stability of the training depending on the optimizer. Optimizers whose step size is dependent on the magnitude of the gradient, like `optimizers.SGD`, may fail. The optimizer used here, `optimizers.Adam`, is unaffected by the scaling change. Also note that because of the weighting, the total losses are not comparable between the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJ589fn8ST3x",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "26/26 [==============================] - 2s 78ms/step - loss: 2.1891 - tp: 11759.0000 - fp: 75818.0000 - tn: 149237.0000 - fn: 14060.0000 - accuracy: 0.6417 - precision: 0.1343 - recall: 0.4554 - auc: 0.5861 - val_loss: 0.7866 - val_tp: 2705.0000 - val_fp: 13799.0000 - val_tn: 18471.0000 - val_fn: 943.0000 - val_accuracy: 0.5896 - val_precision: 0.1639 - val_recall: 0.7415 - val_auc: 0.7124\n",
      "Epoch 2/100\n",
      "26/26 [==============================] - 1s 46ms/step - loss: 0.6648 - tp: 15804.0000 - fp: 75048.0000 - tn: 150007.0000 - fn: 10015.0000 - accuracy: 0.6609 - precision: 0.1740 - recall: 0.6121 - auc: 0.6960 - val_loss: 0.5429 - val_tp: 2441.0000 - val_fp: 9842.0000 - val_tn: 22428.0000 - val_fn: 1207.0000 - val_accuracy: 0.6924 - val_precision: 0.1987 - val_recall: 0.6691 - val_auc: 0.7453\n",
      "Epoch 3/100\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 0.6015 - tp: 18103.0000 - fp: 79277.0000 - tn: 145778.0000 - fn: 7716.0000 - accuracy: 0.6532 - precision: 0.1859 - recall: 0.7012 - auc: 0.7348 - val_loss: 0.5221 - val_tp: 2380.0000 - val_fp: 9301.0000 - val_tn: 22969.0000 - val_fn: 1268.0000 - val_accuracy: 0.7057 - val_precision: 0.2037 - val_recall: 0.6524 - val_auc: 0.7495\n",
      "Epoch 4/100\n",
      "26/26 [==============================] - 1s 56ms/step - loss: 0.5892 - tp: 18900.0000 - fp: 80506.0000 - tn: 144549.0000 - fn: 6919.0000 - accuracy: 0.6515 - precision: 0.1901 - recall: 0.7320 - auc: 0.7476 - val_loss: 0.5145 - val_tp: 2316.0000 - val_fp: 9004.0000 - val_tn: 23266.0000 - val_fn: 1332.0000 - val_accuracy: 0.7122 - val_precision: 0.2046 - val_recall: 0.6349 - val_auc: 0.7509\n",
      "Epoch 5/100\n",
      "26/26 [==============================] - 1s 47ms/step - loss: 0.5871 - tp: 19107.0000 - fp: 81804.0000 - tn: 143251.0000 - fn: 6712.0000 - accuracy: 0.6472 - precision: 0.1893 - recall: 0.7400 - auc: 0.7488 - val_loss: 0.5043 - val_tp: 2258.0000 - val_fp: 8582.0000 - val_tn: 23688.0000 - val_fn: 1390.0000 - val_accuracy: 0.7224 - val_precision: 0.2083 - val_recall: 0.6190 - val_auc: 0.7522\n",
      "Epoch 6/100\n",
      "26/26 [==============================] - 1s 46ms/step - loss: 0.5851 - tp: 19135.0000 - fp: 80850.0000 - tn: 144205.0000 - fn: 6684.0000 - accuracy: 0.6511 - precision: 0.1914 - recall: 0.7411 - auc: 0.7521 - val_loss: 0.4923 - val_tp: 2226.0000 - val_fp: 8256.0000 - val_tn: 24014.0000 - val_fn: 1422.0000 - val_accuracy: 0.7306 - val_precision: 0.2124 - val_recall: 0.6102 - val_auc: 0.7532\n",
      "Epoch 7/100\n",
      "26/26 [==============================] - 1s 45ms/step - loss: 0.5838 - tp: 19063.0000 - fp: 79590.0000 - tn: 145465.0000 - fn: 6756.0000 - accuracy: 0.6558 - precision: 0.1932 - recall: 0.7383 - auc: 0.7535 - val_loss: 0.5107 - val_tp: 2062.0000 - val_fp: 7787.0000 - val_tn: 24483.0000 - val_fn: 1586.0000 - val_accuracy: 0.7390 - val_precision: 0.2094 - val_recall: 0.5652 - val_auc: 0.7503\n",
      "Epoch 8/100\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 0.5840 - tp: 19090.0000 - fp: 80562.0000 - tn: 144493.0000 - fn: 6729.0000 - accuracy: 0.6521 - precision: 0.1916 - recall: 0.7394 - auc: 0.7529 - val_loss: 0.4738 - val_tp: 1954.0000 - val_fp: 7003.0000 - val_tn: 25267.0000 - val_fn: 1694.0000 - val_accuracy: 0.7579 - val_precision: 0.2182 - val_recall: 0.5356 - val_auc: 0.7533\n",
      "Epoch 9/100\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.5823 - tp: 19223.0000 - fp: 80672.0000 - tn: 144383.0000 - fn: 6596.0000 - accuracy: 0.6521 - precision: 0.1924 - recall: 0.7445 - auc: 0.7548 - val_loss: 0.4757 - val_tp: 1926.0000 - val_fp: 7058.0000 - val_tn: 25212.0000 - val_fn: 1722.0000 - val_accuracy: 0.7556 - val_precision: 0.2144 - val_recall: 0.5280 - val_auc: 0.7496\n",
      "Epoch 10/100\n",
      "26/26 [==============================] - 1s 48ms/step - loss: 0.5808 - tp: 19315.0000 - fp: 80859.0000 - tn: 144196.0000 - fn: 6504.0000 - accuracy: 0.6518 - precision: 0.1928 - recall: 0.7481 - auc: 0.7564 - val_loss: 0.4360 - val_tp: 1459.0000 - val_fp: 4832.0000 - val_tn: 27438.0000 - val_fn: 2189.0000 - val_accuracy: 0.8045 - val_precision: 0.2319 - val_recall: 0.3999 - val_auc: 0.7464\n",
      "Epoch 11/100\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 0.5819 - tp: 19251.0000 - fp: 80079.0000 - tn: 144976.0000 - fn: 6568.0000 - accuracy: 0.6546 - precision: 0.1938 - recall: 0.7456 - auc: 0.7559 - val_loss: 0.4337 - val_tp: 1260.0000 - val_fp: 3913.0000 - val_tn: 28357.0000 - val_fn: 2388.0000 - val_accuracy: 0.8246 - val_precision: 0.2436 - val_recall: 0.3454 - val_auc: 0.7489\n",
      "Epoch 12/100\n",
      "26/26 [==============================] - 1s 45ms/step - loss: 0.5788 - tp: 19325.0000 - fp: 80181.0000 - tn: 144874.0000 - fn: 6494.0000 - accuracy: 0.6545 - precision: 0.1942 - recall: 0.7485 - auc: 0.7581 - val_loss: 0.4427 - val_tp: 1387.0000 - val_fp: 4441.0000 - val_tn: 27829.0000 - val_fn: 2261.0000 - val_accuracy: 0.8134 - val_precision: 0.2380 - val_recall: 0.3802 - val_auc: 0.7494\n",
      "Epoch 13/100\n",
      "26/26 [==============================] - 1s 45ms/step - loss: 0.5803 - tp: 19410.0000 - fp: 80539.0000 - tn: 144516.0000 - fn: 6409.0000 - accuracy: 0.6534 - precision: 0.1942 - recall: 0.7518 - auc: 0.7577 - val_loss: 0.4472 - val_tp: 1487.0000 - val_fp: 5034.0000 - val_tn: 27236.0000 - val_fn: 2161.0000 - val_accuracy: 0.7997 - val_precision: 0.2280 - val_recall: 0.4076 - val_auc: 0.7476\n",
      "Epoch 14/100\n",
      "26/26 [==============================] - 1s 45ms/step - loss: 0.5787 - tp: 19545.0000 - fp: 81817.0000 - tn: 143238.0000 - fn: 6274.0000 - accuracy: 0.6489 - precision: 0.1928 - recall: 0.7570 - auc: 0.7582 - val_loss: 0.4177 - val_tp: 1232.0000 - val_fp: 3905.0000 - val_tn: 28365.0000 - val_fn: 2416.0000 - val_accuracy: 0.8240 - val_precision: 0.2398 - val_recall: 0.3377 - val_auc: 0.7449\n",
      "Epoch 15/100\n",
      "26/26 [==============================] - 2s 66ms/step - loss: 0.5798 - tp: 19292.0000 - fp: 79407.0000 - tn: 145648.0000 - fn: 6527.0000 - accuracy: 0.6575 - precision: 0.1955 - recall: 0.7472 - auc: 0.7583 - val_loss: 0.4697 - val_tp: 1605.0000 - val_fp: 5543.0000 - val_tn: 26727.0000 - val_fn: 2043.0000 - val_accuracy: 0.7888 - val_precision: 0.2245 - val_recall: 0.4400 - val_auc: 0.7461\n",
      "Epoch 16/100\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 0.5779 - tp: 19556.0000 - fp: 81619.0000 - tn: 143436.0000 - fn: 6263.0000 - accuracy: 0.6497 - precision: 0.1933 - recall: 0.7574 - auc: 0.7593 - val_loss: 0.4632 - val_tp: 1285.0000 - val_fp: 4113.0000 - val_tn: 28157.0000 - val_fn: 2363.0000 - val_accuracy: 0.8197 - val_precision: 0.2381 - val_recall: 0.3522 - val_auc: 0.7453\n",
      "Epoch 17/100\n",
      "26/26 [==============================] - 2s 80ms/step - loss: 0.5787 - tp: 19446.0000 - fp: 80384.0000 - tn: 144671.0000 - fn: 6373.0000 - accuracy: 0.6542 - precision: 0.1948 - recall: 0.7532 - auc: 0.7592 - val_loss: 0.4121 - val_tp: 986.0000 - val_fp: 2794.0000 - val_tn: 29476.0000 - val_fn: 2662.0000 - val_accuracy: 0.8481 - val_precision: 0.2608 - val_recall: 0.2703 - val_auc: 0.7502\n",
      "Epoch 18/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.5755 - tp: 19422.0000 - fp: 79553.0000 - tn: 144718.0000 - fn: 6307.0000 - accuracy: 0.6566 - precision: 0.1962 - recall: 0.7549 - auc: 0.7618Restoring model weights from the end of the best epoch.\n",
      "26/26 [==============================] - 2s 74ms/step - loss: 0.5757 - tp: 19490.0000 - fp: 79861.0000 - tn: 145194.0000 - fn: 6329.0000 - accuracy: 0.6564 - precision: 0.1962 - recall: 0.7549 - auc: 0.7616 - val_loss: 0.4555 - val_tp: 1314.0000 - val_fp: 4401.0000 - val_tn: 27869.0000 - val_fn: 2334.0000 - val_accuracy: 0.8125 - val_precision: 0.2299 - val_recall: 0.3602 - val_auc: 0.7414\n",
      "Epoch 00018: early stopping\n"
     ]
    }
   ],
   "source": [
    "weighted_model = make_model()\n",
    "weighted_model.load_weights(initial_weights)\n",
    "\n",
    "weighted_history = weighted_model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks = [early_stopping],\n",
    "    validation_data=(val_features, val_labels),\n",
    "    # The class weights go here\n",
    "    class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R0ynYRO0G3Lx"
   },
   "source": [
    "### Check training history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "REy6WClTZIwQ"
   },
   "source": [
    "### Evaluate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_weighted = weighted_model.predict(test_features, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_weighted = pd.DataFrame(test_predictions_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_weighted.to_csv (os.getcwd() + '/' + 'predictions.csv', index = False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nifqscPGw-5w"
   },
   "outputs": [],
   "source": [
    "train_predictions_weighted = weighted_model.predict(train_features, batch_size=BATCH_SIZE)\n",
    "# val_predictions_weighted = weighted_model.predict(val_features, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hXDAwyr0HYdX"
   },
   "source": [
    "### Plot the ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3hzScIVZS1Xm"
   },
   "outputs": [],
   "source": [
    "plot_roc(\"Train Weighted\", train_labels, train_predictions_weighted, color=colors[1])\n",
    "plot_roc(\"Validation Weighted\", val_labels, val_predictions_weighted, color=colors[1], linestyle='--')\n",
    "\n",
    "\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Precision Report:\")\n",
    "print(\"Validation Weighted: \", average_precision_score(val_labels, val_predictions_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC:\")\n",
    "print(\"Validation Weighted: \", AUCcalc(val_labels, val_predictions_weighted))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "imbalanced_data.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
